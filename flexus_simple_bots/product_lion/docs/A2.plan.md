# Productman A2 Integration Plan

## Overview

Implement full A2 activity (Problem Hypothesis Generation & Prioritization) with canonical formulation format, ICE prioritization matrix, Challenge Loop workflow, and validation.

**Scope**: Must Have + Should Have from analysis

**Approach**:

- LLM-based hypothesis generation and scoring
- Breaking change acceptable (new workflow)
- Folder structure: `/customer-research/{idea-name}/hypotheses/problem-list`

---

## Phase 1: Create Problem Hypothesis List Structure

### 1.1 Create Problem Hypothesis List Tool

**File**: `flexus_simple_bots/productman/productman_bot.py`

Add new tool after VALIDATE_ARTIFACT_TOOL:

```python
GENERATE_PROBLEM_HYPOTHESES_TOOL = ckit_cloudtool.CloudTool(
    name="generate_problem_hypotheses",
    description="Generate Problem Hypothesis List from Idea Framing Sheet. Extracts core problems and assumptions, formulates testable hypotheses. Path: /customer-research/{idea-name}/hypotheses/problem-list",
    parameters={
        "type": "object",
        "properties": {
            "idea_name": {
                "type": "string",
                "description": "Idea name in kebab-case (e.g. 'slack-microwave')"
            },
            "sheet_path": {
                "type": "string",
                "description": "Path to Idea Framing Sheet (optional, defaults to /customer-research/{idea-name}/sheet)"
            }
        },
        "required": ["idea_name"]
    }
)
```

Update TOOLS list (line 92):

```python
TOOLS = [
    FIRST_PRINCIPLES_CANVAS_TOOL,
    IDEA_FRAMING_SHEET_TOOL,
    GENERATE_PROBLEM_HYPOTHESES_TOOL,
    HYPOTHESIS_TEMPLATE_TOOL,  # Keep for solution hypotheses (A3)
    VALIDATE_ARTIFACT_TOOL,
    fi_pdoc.POLICY_DOCUMENT_TOOL,
]
```

### 1.2 Implement Handler for Problem Hypothesis Generation

**File**: `flexus_simple_bots/productman/productman_bot.py`

Add handler after toolcall_create_idea_framing_sheet:

```python
@rcx.on_tool_call(GENERATE_PROBLEM_HYPOTHESES_TOOL.name)
async def toolcall_generate_problem_hypotheses(toolcall: ckit_cloudtool.FCloudtoolCall, model_produced_args: Dict[str, Any]) -> str:
    from datetime import datetime
    
    idea_name = model_produced_args.get("idea_name", "")
    sheet_path = model_produced_args.get("sheet_path", f"/customer-research/{idea_name}/sheet")
    
    if not idea_name:
        return "Error: idea_name required"
    
    # Validate kebab-case
    if not all(c.islower() or c.isdigit() or c == "-" for c in idea_name):
        return f"Error: idea_name '{idea_name}' must use kebab-case"
    
    # Read Idea Framing Sheet
    try:
        sheet_content = await pdoc_integration.pdoc_read(sheet_path, toolcall.fcall_ft_id)
        sheet_data = json.loads(sheet_content)
    except Exception as e:
        return f"Error reading Idea Framing Sheet: {str(e)}"
    
    # Extract relevant fields
    target_segment = sheet_data.get("target_segment", {})
    core_problem = sheet_data.get("core_problem", {})
    key_assumptions = sheet_data.get("key_assumptions", [])
    constraints = sheet_data.get("constraints", [])
    
    # Construct generation prompt
    generation_prompt = f"""Generate 3-5 problem hypotheses from this Idea Framing Sheet.

Target Segment:
{json.dumps(target_segment, indent=2)}

Core Problem:
{json.dumps(core_problem, indent=2)}

Key Assumptions:
{json.dumps(key_assumptions, indent=2)}

Constraints:
{json.dumps(constraints, indent=2)}

Generate hypotheses in this EXACT format:
"Our customer [specific segment] wants [outcome goal], but cannot [action], because [single reason]"

Rules:
- Each hypothesis has ONLY ONE reason (no "and", no multiple assumptions)
- Goal is outcome, not method (e.g. "achieve 5% response rate", not "use AI")
- Reason is testable/falsifiable (can design experiment)
- Cover different angles: time, skill, access, cost

Return JSON array:
[
  {{
    "hypothesis_id": "H1",
    "formulation": "Our customer...",
    "segment": "extracted segment",
    "goal": "extracted goal",
    "barrier": "extracted barrier",
    "reason": "extracted reason"
  }},
  ...
]"""
    
    try:
        # Generate hypotheses via LLM
        hypotheses_result = await rcx.ask_model_raw(
            prompt=generation_prompt,
            model_name="gpt-4o",
        )
        
        # Parse hypotheses
        hypotheses_list = json.loads(hypotheses_result)
        
        # Construct Problem Hypothesis List structure
        problem_list = {
            "source_idea_framing_sheet_id": sheet_path,
            "hypotheses": hypotheses_list,
            "prioritization_date": None,  # Will be set after prioritization
            "selected_hypothesis_id": None,
            "meta": {
                "created": datetime.utcnow().isoformat(),
                "version": "v0"
            }
        }
        
        # Write to path
        output_path = f"/customer-research/{idea_name}/hypotheses/problem-list"
        await pdoc_integration.pdoc_write(output_path, json.dumps(problem_list, indent=2), toolcall.fcall_ft_id)
        
        logger.info(f"Generated {len(hypotheses_list)} problem hypotheses at {output_path}")
        
        # Format response
        hypotheses_text = "\n".join([f"{h['hypothesis_id']}: {h['formulation']}" for h in hypotheses_list])
        return f"Problem Hypothesis List created at {output_path}\n\nGenerated {len(hypotheses_list)} hypotheses:\n{hypotheses_text}\n\nNext steps:\n1. Review hypotheses for clarity\n2. Optional: Challenge hypotheses (coming in Phase 2)\n3. Prioritize hypotheses using prioritize_hypotheses tool"
        
    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse hypotheses: {e}")
        return f"Error: Failed to parse hypotheses (got non-JSON response). Try again."
    except Exception as e:
        logger.error(f"Error generating hypotheses: {e}")
        return f"Error generating hypotheses: {str(e)}"
```

---

## Phase 2: Add Prioritization Tool

### 2.1 Create Prioritize Hypotheses Tool

**File**: `flexus_simple_bots/productman/productman_bot.py`

Add tool definition:

```python
PRIORITIZE_HYPOTHESES_TOOL = ckit_cloudtool.CloudTool(
    name="prioritize_hypotheses",
    description="Prioritize problem hypotheses using ICE matrix (Impact × Evidence × Feasibility). Scores each hypothesis 0-5 on each criterion, calculates weighted score.",
    parameters={
        "type": "object",
        "properties": {
            "problem_list_path": {
                "type": "string",
                "description": "Path to Problem Hypothesis List (e.g. /customer-research/slack-microwave/hypotheses/problem-list)"
            }
        },
        "required": ["problem_list_path"]
    }
)
```

Add to TOOLS list.

### 2.2 Implement Prioritization Handler

**File**: `flexus_simple_bots/productman/productman_bot.py`

Add handler:

```python
@rcx.on_tool_call(PRIORITIZE_HYPOTHESES_TOOL.name)
async def toolcall_prioritize_hypotheses(toolcall: ckit_cloudtool.FCloudtoolCall, model_produced_args: Dict[str, Any]) -> str:
    from datetime import datetime
    
    problem_list_path = model_produced_args.get("problem_list_path", "")
    
    if not problem_list_path:
        return "Error: problem_list_path required"
    
    # Read Problem Hypothesis List
    try:
        list_content = await pdoc_integration.pdoc_read(problem_list_path, toolcall.fcall_ft_id)
        problem_list = json.loads(list_content)
    except Exception as e:
        return f"Error reading Problem Hypothesis List: {str(e)}"
    
    hypotheses = problem_list.get("hypotheses", [])
    
    if not hypotheses:
        return "Error: No hypotheses found in list"
    
    # Get ICE scoring criteria from prompts module
    ice_criteria = productman_prompts.ICE_PRIORITIZATION_CRITERIA
    
    # Score each hypothesis
    scored_hypotheses = []
    
    for hyp in hypotheses:
        scoring_prompt = f"""Score this problem hypothesis using ICE matrix (Impact × Evidence × Feasibility).

Hypothesis: {hyp.get('formulation', '')}

Criteria:
{ice_criteria}

Return JSON:
{{
  "impact": 0-5,
  "evidence": 0-5,
  "feasibility": 0-5,
  "reasoning": {{
    "impact": "why this score",
    "evidence": "why this score",
    "feasibility": "why this score"
  }}
}}"""
        
        try:
            score_result = await rcx.ask_model_raw(
                prompt=scoring_prompt,
                model_name="gpt-4o-mini",
            )
            
            scores = json.loads(score_result)
            
            # Calculate weighted score
            weighted_score = (
                0.4 * scores.get("impact", 0) +
                0.4 * scores.get("evidence", 0) +
                0.2 * scores.get("feasibility", 0)
            )
            
            # Update hypothesis with scores
            hyp["priority_scores"] = {
                "impact": scores.get("impact", 0),
                "evidence": scores.get("evidence", 0),
                "feasibility": scores.get("feasibility", 0),
                "weighted_score": round(weighted_score, 2),
                "reasoning": scores.get("reasoning", {})
            }
            
            scored_hypotheses.append(hyp)
            
        except Exception as e:
            logger.error(f"Failed to score {hyp.get('hypothesis_id', '?')}: {e}")
            # Add default scores if scoring fails
            hyp["priority_scores"] = {
                "impact": 3,
                "evidence": 3,
                "feasibility": 3,
                "weighted_score": 3.0,
                "reasoning": {"error": str(e)}
            }
            scored_hypotheses.append(hyp)
    
    # Sort by weighted_score descending
    scored_hypotheses.sort(key=lambda h: h.get("priority_scores", {}).get("weighted_score", 0), reverse=True)
    
    # Update problem list
    problem_list["hypotheses"] = scored_hypotheses
    problem_list["prioritization_date"] = datetime.utcnow().isoformat()
    
    # Write back
    await pdoc_integration.pdoc_write(problem_list_path, json.dumps(problem_list, indent=2), toolcall.fcall_ft_id)
    
    logger.info(f"Prioritized {len(scored_hypotheses)} hypotheses at {problem_list_path}")
    
    # Format response as table
    response = f"Prioritization complete!\n\nTop 3 Problem Hypotheses:\n\n"
    
    for i, hyp in enumerate(scored_hypotheses[:3], 1):
        scores = hyp.get("priority_scores", {})
        response += f"{i}. {hyp.get('hypothesis_id', '?')} (Score: {scores.get('weighted_score', 0)})\n"
        response += f"   Impact: {scores.get('impact', 0)}, Evidence: {scores.get('evidence', 0)}, Feasibility: {scores.get('feasibility', 0)}\n"
        response += f"   {hyp.get('formulation', '')}\n\n"
    
    response += f"Next steps:\n"
    response += f"1. Review top hypotheses\n"
    response += f"2. Optional: Validate hypothesis list\n"
    response += f"3. Select one hypothesis for solution design (A3)"
    
    return response
```

---

## Phase 3: Update Prompt with A2 Methodology

### 3.1 Add Problem Hypothesis Formulation Rules

**File**: `flexus_simple_bots/productman/productman_prompts.py`

Add after FIRST_PRINCIPLES_CANVAS_VALIDATION_CRITERIA:

```python
# Problem Hypothesis Formulation Rules from controls/problem-hypothesis-formulation-rules.md
PROBLEM_HYPOTHESIS_FORMULATION_RULES = """
## Canonical Format

"Our customer [SEGMENT] wants [GOAL], but cannot [ACTION], because [REASON]"

## Rules

Rule 1: Single Assumption
- Each hypothesis contains ONLY ONE reason/assumption
- BAD: "...because they lack time and budget"
- GOOD: Split into two hypotheses

Rule 2: Goal is Outcome, Not Method
- BAD: "wants to use AI for GTM" (method)
- GOOD: "wants to launch campaigns in <1 week" (outcome)

Rule 3: Testability
- The reason must be testable/falsifiable
- BAD: "...because they're lazy" (subjective)
- GOOD: "...because it requires >10 hours of manual work" (measurable)

Rule 4: Specificity
- BAD: "Founders want better sales" (vague)
- GOOD: "Pre-seed B2B SaaS founders want to achieve 5% email response rates, but cannot craft effective messages, because they lack copywriting expertise"

## Examples

✅ GOOD:
- "Early-stage B2B SaaS founders want to launch their first outbound campaign within 2 weeks, but cannot build a qualified lead list fast enough, because they lack access to quality B2B databases"

❌ BAD:
- "Founders want to grow faster" (too vague)
- "Founders want AI to do their GTM" (goal is method, not outcome)
- "Founders can't launch campaigns because they're overwhelmed and lack focus" (two reasons, not falsifiable)
"""

# ICE Prioritization Criteria from controls/prioritization-matrix-ice.md
ICE_PRIORITIZATION_CRITERIA = """
## ICE Matrix Scoring (0-5 scale)

### Impact (0-5)
Question: If this problem is real and we solve it, how significant is the effect?

Scale:
- 5 (Critical): Top-3 pain point, customer would pay premium, mission-critical
- 4 (High): Important pain, strong willingness to pay
- 3 (Medium): Noticeable improvement, would consider paying
- 2 (Low): Nice-to-have, unlikely to pay much
- 1 (Minimal): Barely noticeable, wouldn't pay
- 0 (None): No real impact

### Evidence (0-5)
Question: How much evidence exists that this problem is real and widespread?

Scale:
- 5 (Strong): Multiple data sources, quantitative research, competitor validation
- 4 (Good): Some data + multiple anecdotes, market reports
- 3 (Moderate): Mix of assumptions + some evidence
- 2 (Weak): Mostly assumptions, 1-2 anecdotal mentions
- 1 (Minimal): Pure speculation
- 0 (None): Contradicts available evidence

### Feasibility (0-5)
Question: How easy to TEST (not solve) this hypothesis?

Scale:
- 5 (Very Easy): < 1 week, < $100
- 4 (Easy): 1-2 weeks, < $500
- 3 (Moderate): 2-4 weeks, $500-$1000
- 2 (Hard): 4-8 weeks, $1000-$5000
- 1 (Very Hard): > 8 weeks, > $5000
- 0 (Infeasible): Cannot test

## Weighted Formula

PriorityScore = 0.4 × Impact + 0.4 × Evidence + 0.2 × Feasibility

Output Range: 0-5 (5 = highest priority)
"""

PROBLEM_HYPOTHESIS_LIST_VALIDATION_CRITERIA = """
CRITICAL CRITERIA (fail if violated):
- C1: Each hypothesis follows exact format "Our customer [segment] wants [goal], but cannot [action], because [reason]"
- C2: Each hypothesis contains ONLY ONE reason (no "and", no multiple assumptions)
- C3: Reason is testable/falsifiable (can design experiment to verify)
- C4: No duplicate hypotheses (same segment + goal + barrier)

WARNING CRITERIA (pass-with-warnings):
- W1: Fewer than 3 hypotheses in list
- W2: No research_evidence provided for any hypothesis
- W3: All priority scores are identical (suspicious uniformity)
- W4: No selected_hypothesis_id (if validation happens after selection)
"""
```

### 3.2 Add A2 Workflow Section to Prompt

**File**: `flexus_simple_bots/productman/productman_prompts.py`

Update productman_prompt (after A1 workflow, before "Future Phases"):

```python
## A2 Workflow: Problem Hypothesis Generation & Prioritization

### Step 1 (A21): Extract Problem Hypotheses from Idea Framing Sheet

After Sheet is validated in A1, generate problem hypotheses:

Use generate_problem_hypotheses(idea_name="...")

Process:
1. Read Idea Framing Sheet (core_problem, key_assumptions, target_segment, constraints)
2. Generate 3-7 hypotheses covering different angles:
   - Time barrier: "...cannot do X fast enough, because it takes >10 hours"
   - Skill barrier: "...cannot craft Y, because they lack expertise in Z"
   - Access barrier: "...cannot access W, because they don't have premium databases"
   - Cost barrier: "...cannot afford V, because budget is <$500/month"
3. Each hypothesis follows STRICT format:
   "Our customer [specific segment] wants [outcome goal], but cannot [action], because [single reason]"

**Rules:**
{PROBLEM_HYPOTHESIS_FORMULATION_RULES}

**Example Dialog:**
User: "Generate problem hypotheses from slack-microwave Sheet"
You: [reads Sheet] "I've identified 4 key problems. Generating hypotheses..."
[creates problem-list with H1-H4]
You: "Created 4 problem hypotheses:
H1: Pre-seed B2B founders want to launch campaigns in <2 weeks, but cannot build lead lists fast, because they lack B2B databases
H2: Technical founders want 5%+ response rates, but cannot craft effective emails, because they lack copywriting expertise
...
Next: prioritize these hypotheses to find which problem to focus on."

### Step 2 (A22): Challenge Loop (Optional, for quality improvement)

For each hypothesis, challenge formulation by creating counterexample:

**How Challenge Loop works:**
1. Take hypothesis: "Founders want faster campaigns"
2. Create counterexample: "A service that launches campaigns in 1 second by sending spam to random people"
3. Reveal issue: "Faster" without quality constraint is meaningless
4. Refine: "Founders want to launch qualified, targeted campaigns in <1 week, but cannot build lead lists fast, because they lack accurate B2B contact data"

Log each iteration in challenge_log.

**When to use:**
- Hypothesis seems vague or too broad
- User wants to sharpen formulation
- Before prioritization (optional but recommended)

### Step 3 (A23): Prioritize Hypotheses

After hypotheses are generated (and optionally challenged), prioritize using ICE matrix:

Use prioritize_hypotheses(problem_list_path="/customer-research/{idea-name}/hypotheses/problem-list")

**ICE Matrix:**
{ICE_PRIORITIZATION_CRITERIA}

For each hypothesis, score:
- **Impact (0-5)**: How significant is this problem if real?
- **Evidence (0-5)**: How much proof exists that problem is widespread?
- **Feasibility (0-5)**: How easy to TEST (not solve) this hypothesis?

**Weighted Formula**: PriorityScore = 0.4×Impact + 0.4×Evidence + 0.2×Feasibility

Output: Ranked list with top 3 hypotheses highlighted.

**Example Output:**
```
Top 3 Problem Hypotheses:

1. H1 (Score: 4.6)
   Impact: 5, Evidence: 4, Feasibility: 5
   "Pre-seed B2B SaaS founders want to launch first campaign in <2 weeks,
    but cannot build qualified lead lists fast, because they lack B2B databases"

2. H3 (Score: 4.2)
   Impact: 4, Evidence: 5, Feasibility: 3
   ...

Recommendation: Test H1 first (highest score, easiest to execute)
```

### Step 4 (A24): Validate and Select Hypothesis

After prioritization, validate list quality:

Use validate_artifact(artifact_path="...", artifact_type="problem-hypothesis-list")

**Validation checks:**
{PROBLEM_HYPOTHESIS_LIST_VALIDATION_CRITERIA}

**If PASS:**
Present top 3 to user: "Top 3 hypotheses ranked. Which would you like to test first? (Recommendation: H1 with score 4.6)"

**If FAIL:**
List issues, help fix, re-validate.

**User selects hypothesis:**
Update problem-list: selected_hypothesis_id = "H1", validation_status = "selected"

Tell user: "H1 selected! Ready to move to A3 (solution hypothesis generation). This is where we design ways to TEST the selected problem."

---

## Path Structure (Updated)

/customer-research/{idea-name}/canvas                        # First Principles Canvas (A1)
/customer-research/{idea-name}/sheet                         # Idea Framing Sheet (A1)
/customer-research/{idea-name}/hypotheses/problem-list       # Problem Hypothesis List (A2) ← NEW
/customer-research/{idea-name}/hypotheses/{solution-name}    # Solution Hypotheses (future A3)
/customer-research/{idea-name}/surveys/...                   # Surveys (future A4-A6)

---

## Example Problem Hypothesis List

{json.dumps(example_problem_hypothesis_list, indent=2)}
```

### 3.3 Add Example Problem Hypothesis List

**File**: `flexus_simple_bots/productman/productman_prompts.py`

Add after example_sheet:

```python
example_problem_hypothesis_list = {
    "source_idea_framing_sheet_id": "/customer-research/gtm-automation/sheet",
    "hypotheses": [
        {
            "hypothesis_id": "H1",
            "formulation": "Our customer (pre-seed B2B SaaS founders, 1-10 employees) wants to launch their first outbound campaign within 2 weeks, but cannot build a qualified lead list fast enough, because they lack access to quality B2B contact databases",
            "segment": "Pre-seed B2B SaaS founders, 1-10 employees",
            "goal": "Launch first outbound campaign within 2 weeks",
            "barrier": "Cannot build qualified lead list fast enough",
            "reason": "Lack access to quality B2B contact databases",
            "challenge_log": [
                {
                    "iteration": 1,
                    "counterexample": "What if we give them free access to Apollo? They still won't launch fast.",
                    "refinement": "Added 'qualified' - problem is not just access but also knowing how to filter/qualify leads"
                }
            ],
            "priority_scores": {
                "impact": 5,
                "evidence": 4,
                "feasibility": 5,
                "weighted_score": 4.6,
                "reasoning": {
                    "impact": "Launch speed critical for early traction, direct impact on revenue timing",
                    "evidence": "Multiple founder interviews + competitor positioning (Apollo, Clay)",
                    "feasibility": "Can survey founders via SurveyMonkey or LinkedIn, <$500 budget"
                }
            },
            "research_evidence": [
                {
                    "source_url": "https://example.com/founder-survey-2024",
                    "snippet": "78% of pre-seed founders cite list building as #1 time sink in outbound"
                }
            ],
            "validation_status": "selected"
        },
        {
            "hypothesis_id": "H2",
            "formulation": "Our customer (solo technical founders) wants to achieve 5%+ email response rates, but cannot craft effective cold email copy, because they lack copywriting expertise and proven templates",
            "segment": "Solo technical founders",
            "goal": "Achieve 5%+ email response rates",
            "barrier": "Cannot craft effective cold email copy",
            "reason": "Lack copywriting expertise and proven templates",
            "challenge_log": [],
            "priority_scores": {
                "impact": 4,
                "evidence": 3,
                "feasibility": 4,
                "weighted_score": 3.6,
                "reasoning": {
                    "impact": "Response rate directly impacts conversion, high importance",
                    "evidence": "Anecdotal from founder communities, no hard data",
                    "feasibility": "Easy to test via survey about current response rates"
                }
            },
            "research_evidence": [],
            "validation_status": "validated"
        }
    ],
    "prioritization_date": "2025-11-04T10:30:00Z",
    "selected_hypothesis_id": "H1",
    "meta": {
        "created": "2025-11-04T09:00:00Z",
        "version": "v0"
    }
}
```

---

## Phase 4: Add Challenge Loop (Optional)

### 4.1 Create Challenge Hypothesis Tool

**File**: `flexus_simple_bots/productman/productman_bot.py`

Add tool definition:

```python
CHALLENGE_HYPOTHESIS_TOOL = ckit_cloudtool.CloudTool(
    name="challenge_hypothesis",
    description="Challenge a hypothesis by creating a counterexample to reveal gaps in formulation. Improves hypothesis quality.",
    parameters={
        "type": "object",
        "properties": {
            "problem_list_path": {
                "type": "string",
                "description": "Path to Problem Hypothesis List"
            },
            "hypothesis_id": {
                "type": "string",
                "description": "ID of hypothesis to challenge (e.g. 'H1')"
            }
        },
        "required": ["problem_list_path", "hypothesis_id"]
    }
)
```

Add to TOOLS list.

### 4.2 Implement Challenge Handler

**File**: `flexus_simple_bots/productman/productman_bot.py`

Add handler:

```python
@rcx.on_tool_call(CHALLENGE_HYPOTHESIS_TOOL.name)
async def toolcall_challenge_hypothesis(toolcall: ckit_cloudtool.FCloudtoolCall, model_produced_args: Dict[str, Any]) -> str:
    problem_list_path = model_produced_args.get("problem_list_path", "")
    hypothesis_id = model_produced_args.get("hypothesis_id", "")
    
    if not problem_list_path or not hypothesis_id:
        return "Error: problem_list_path and hypothesis_id required"
    
    # Read problem list
    try:
        list_content = await pdoc_integration.pdoc_read(problem_list_path, toolcall.fcall_ft_id)
        problem_list = json.loads(list_content)
    except Exception as e:
        return f"Error reading problem list: {str(e)}"
    
    # Find hypothesis
    hypotheses = problem_list.get("hypotheses", [])
    target_hyp = next((h for h in hypotheses if h.get("hypothesis_id") == hypothesis_id), None)
    
    if not target_hyp:
        return f"Error: Hypothesis {hypothesis_id} not found"
    
    formulation = target_hyp.get("formulation", "")
    
    # Generate counterexample
    challenge_prompt = f"""Challenge this hypothesis by creating a counterexample.

Hypothesis: {formulation}

Your task:
1. Design a business/service that LITERALLY fulfills the hypothesis
2. But leads to OPPOSITE or ABSURD result
3. Reveal what's missing or vague in the formulation

Example:
Hypothesis: "Founders want faster campaigns"
Counterexample: "A service that launches campaigns in 1 second by sending spam to random people"
Issue Revealed: "Faster" without quality constraint is meaningless

Return JSON:
{{
  "counterexample": "description of business/service",
  "issue_revealed": "what gap this exposes",
  "suggested_refinement": "how to improve formulation"
}}"""
    
    try:
        challenge_result = await rcx.ask_model_raw(
            prompt=challenge_prompt,
            model_name="gpt-4o",
        )
        
        challenge_data = json.loads(challenge_result)
        
        # Add to challenge_log
        if "challenge_log" not in target_hyp:
            target_hyp["challenge_log"] = []
        
        iteration_num = len(target_hyp["challenge_log"]) + 1
        
        target_hyp["challenge_log"].append({
            "iteration": iteration_num,
            "counterexample": challenge_data.get("counterexample", ""),
            "issue_revealed": challenge_data.get("issue_revealed", ""),
            "suggested_refinement": challenge_data.get("suggested_refinement", "")
        })
        
        # Write back
        await pdoc_integration.pdoc_write(problem_list_path, json.dumps(problem_list, indent=2), toolcall.fcall_ft_id)
        
        logger.info(f"Challenged {hypothesis_id}, iteration {iteration_num}")
        
        # Format response
        response = f"Challenge for {hypothesis_id}:\n\n"
        response += f"Counterexample:\n{challenge_data.get('counterexample', '')}\n\n"
        response += f"Issue Revealed:\n{challenge_data.get('issue_revealed', '')}\n\n"
        response += f"Suggested Refinement:\n{challenge_data.get('suggested_refinement', '')}\n\n"
        response += f"Would you like to refine this hypothesis based on the challenge? I can help update the formulation."
        
        return response
        
    except Exception as e:
        logger.error(f"Challenge error: {e}")
        return f"Error during challenge: {str(e)}"
```

---

## Phase 5: Update Validation Tool for Problem Hypothesis List

### 5.1 Update VALIDATE_ARTIFACT_TOOL

**File**: `flexus_simple_bots/productman/productman_bot.py`

Update tool parameters (lines 72-90):

```python
VALIDATE_ARTIFACT_TOOL = ckit_cloudtool.CloudTool(
    name="validate_artifact",
    description="Validate artifact (Canvas, Sheet, or Problem Hypothesis List) against quality criteria. Returns status (pass/pass-with-warnings/fail) with issues and suggestions.",
    parameters={
        "type": "object",
        "properties": {
            "artifact_path": {
                "type": "string",
                "description": "Path to artifact"
            },
            "artifact_type": {
                "type": "string",
                "enum": ["canvas", "sheet", "problem-hypothesis-list"],  # Added problem-hypothesis-list
                "description": "Type of artifact to validate"
            }
        },
        "required": ["artifact_path", "artifact_type"]
    }
)
```

### 5.2 Update Validation Handler

**File**: `flexus_simple_bots/productman/productman_bot.py`

Update toolcall_validate_artifact handler (lines 363-448):

```python
# In existing handler, add case for problem-hypothesis-list
if artifact_type == "sheet":
    criteria = productman_prompts.IDEA_FRAMING_SHEET_VALIDATION_CRITERIA
elif artifact_type == "canvas":
    criteria = productman_prompts.FIRST_PRINCIPLES_CANVAS_VALIDATION_CRITERIA
elif artifact_type == "problem-hypothesis-list":
    criteria = productman_prompts.PROBLEM_HYPOTHESIS_LIST_VALIDATION_CRITERIA
else:
    return f"Error: artifact_type must be 'canvas', 'sheet', or 'problem-hypothesis-list', got '{artifact_type}'"
```

---

## Phase 6: Testing

### Test Case 1: Generate Problem Hypotheses from Sheet

**Input**: User says "Generate problem hypotheses from slack-microwave Sheet"

**Expected**:
- Bot reads Sheet at `/customer-research/slack-microwave/sheet`
- Extracts core_problem, key_assumptions, target_segment
- Generates 3-5 hypotheses in canonical format
- Creates Problem Hypothesis List at `/customer-research/slack-microwave/hypotheses/problem-list`
- Each hypothesis has hypothesis_id, formulation, segment, goal, barrier, reason

### Test Case 2: Prioritize Hypotheses

**Input**: User says "Prioritize problem hypotheses"

**Expected**:
- Bot reads problem-list
- Scores each hypothesis (Impact 0-5, Evidence 0-5, Feasibility 0-5)
- Calculates weighted_score = 0.4×I + 0.4×E + 0.2×F
- Sorts by score descending
- Updates problem-list with priority_scores
- Shows top 3 as table with scores

### Test Case 3: Challenge Hypothesis

**Input**: User says "Challenge hypothesis H1"

**Expected**:
- Bot reads H1 from problem-list
- Generates counterexample (business that literally fulfills hypothesis but leads to absurd result)
- Shows counterexample to user
- Suggests refinement
- Adds entry to challenge_log

### Test Case 4: Validate Problem Hypothesis List

**Input**: User validates problem-list with bad formulation (e.g. H2 has "two reasons")

**Expected**:
- Validation returns FAIL with critical issue: "C2: H2 contains two reasons ('lack time and budget')"
- Suggestion: "Split H2 into two hypotheses: one for time, one for budget"
- Bot asks user to fix and re-validate

### Test Case 5: Select Hypothesis for A3

**Input**: User says "Select H1 for solution design"

**Expected**:
- Bot updates problem-list: selected_hypothesis_id = "H1"
- H1.validation_status = "selected"
- Bot says: "H1 selected! Ready for A3 (solution hypothesis generation)"

---

## Implementation Order

1. Phase 1.1-1.2: Generate Problem Hypotheses tool and handler (2 hours)
2. Phase 2.1-2.2: Prioritization tool and handler (2-3 hours)
3. Phase 3.1-3.3: Prompt update with A2 methodology (2 hours)
4. Phase 5: Update validation tool for problem-hypothesis-list (1 hour)
5. Phase 4: Challenge Loop (optional, 2-3 hours)
6. Phase 6: Testing all cases (1 hour)

**Total Estimated Time**: 8-11 hours (without Challenge Loop: 7-8 hours)

---

## Files Modified

- `flexus_simple_bots/productman/productman_bot.py` (tools, handlers, validation)
- `flexus_simple_bots/productman/productman_prompts.py` (methodology, criteria, examples, A2 workflow)
- `flexus_simple_bots/productman/productman_install.py` (update tool descriptions in marketplace)

## Files Created

- `flexus_simple_bots/productman/docs/productman-a2-analysis.md` (already exists, keep for reference)

### To-dos

- [ ] Create Generate Problem Hypotheses tool and handler with canonical format
- [ ] Create Prioritize Hypotheses tool with ICE matrix scoring
- [ ] Update prompt with Problem Hypothesis Formulation Rules and A2 workflow
- [ ] Update VALIDATE_ARTIFACT_TOOL to support "problem-hypothesis-list" artifact type
- [ ] Add example Problem Hypothesis List to prompts
- [ ] Optional: Create Challenge Hypothesis tool for quality improvement
- [ ] Test all 5 test cases: generation, prioritization, challenge, validation, selection


